{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(f):\n",
    "    def wrap(*args, **kwargs):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args, **kwargs)\n",
    "        time2 = time.time()\n",
    "        print('%s function took %0.3f ms' % (f.__name__, (time2-time1)*1000.0))\n",
    "        return ret\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rewards_and_transitions(problem, mutate=False):\n",
    "    # Enumerate state and action space sizes\n",
    "    num_states = problem.observation_space.n\n",
    "    num_actions = problem.action_space.n\n",
    "\n",
    "    # Initialize Transition and Reward matrices\n",
    "    R = np.zeros((num_states, num_actions, num_states))\n",
    "    T = np.zeros((num_states, num_actions, num_states))\n",
    "\n",
    "    # Iterate over states, actions, and transitions\n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            for transition in problem.env.P[state][action]:\n",
    "                probability, next_state, reward, done = transition\n",
    "                R[state, action, next_state] = reward\n",
    "                T[state, action, next_state] = probability\n",
    "\n",
    "            # Normalize T across state + action axes\n",
    "            T[state, action,:] /= np.sum(T[state, action,:])\n",
    "\n",
    "    # Conditionally mutate and return\n",
    "    if mutate:\n",
    "        problem.env.R = R\n",
    "        problem.env.T = T\n",
    "    return R, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def value_iteration(problem, R=None, T=None, gamma=0.9, max_iterations=10**6, delta=10**-3):\n",
    "    \"\"\" Runs Value Iteration on a gym problem \"\"\"\n",
    "    value_fn = np.zeros(problem.observation_space.n)\n",
    "    if R is None or T is None:\n",
    "        R, T = evaluate_rewards_and_transitions(problem)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        previous_value_fn = value_fn.copy()\n",
    "        Q = np.einsum('ijk,ijk -> ij', T, R + gamma * value_fn)\n",
    "        value_fn = np.max(Q, axis=1)\n",
    "\n",
    "        if np.max(np.abs(value_fn - previous_value_fn)) < delta:\n",
    "            break\n",
    "\n",
    "    # Get and return optimal policy\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    return policy, i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_policy(policy, shape):\n",
    "    \"\"\" One-hot encodes a policy \"\"\"\n",
    "    encoded_policy = np.zeros(shape)\n",
    "    encoded_policy[np.arange(shape[0]), policy] = 1\n",
    "    return encoded_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def policy_iteration(problem, R=None, T=None, gamma=0.9, max_iterations=10**6, delta=10**-3):\n",
    "    \"\"\" Runs Policy Iteration on a gym problem \"\"\"\n",
    "    num_spaces = problem.observation_space.n\n",
    "    num_actions = problem.action_space.n\n",
    "\n",
    "    # Initialize with a random policy and initial value function\n",
    "    policy = np.array([problem.action_space.sample() for _ in range(num_spaces)])\n",
    "    value_fn = np.zeros(num_spaces)\n",
    "\n",
    "    # Get transitions and rewards\n",
    "    if R is None or T is None:\n",
    "        R, T = evaluate_rewards_and_transitions(problem)\n",
    "\n",
    "    # Iterate and improve policies\n",
    "    for i in range(max_iterations):\n",
    "        previous_policy = policy.copy()\n",
    "\n",
    "        for j in range(max_iterations):\n",
    "            previous_value_fn = value_fn.copy()\n",
    "            Q = np.einsum('ijk,ijk -> ij', T, R + gamma * value_fn)\n",
    "            value_fn = np.sum(encode_policy(policy, (num_spaces, num_actions)) * Q, 1)\n",
    "\n",
    "            if np.max(np.abs(previous_value_fn - value_fn)) < delta:\n",
    "                break\n",
    "\n",
    "        Q = np.einsum('ijk,ijk -> ij', T, R + gamma * value_fn)\n",
    "        policy = np.argmax(Q, axis=1)\n",
    "\n",
    "        if np.array_equal(policy, previous_policy):\n",
    "            break\n",
    "\n",
    "    # Return optimal policy\n",
    "    return policy, i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy, mapping=None, shape=(0,)):\n",
    "    print(np.array([mapping[action] for action in policy]).reshape(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_discrete(environment_name, mapping, shape=None , tol=10**-3, env = None):   \n",
    "    if env == None:\n",
    "        problem = gym.make(environment_name)\n",
    "    else:\n",
    "        from gym.wrappers.time_limit import TimeLimit\n",
    "        problem = TimeLimit(env)\n",
    "\n",
    "    print('== {} =='.format(environment_name))\n",
    "    print('Actions:', problem.env.action_space.n)\n",
    "    print('States:', problem.env.observation_space.n)\n",
    "    #print(problem.env.desc)\n",
    "    print()\n",
    "\n",
    "    print('== Value Iteration ==')\n",
    "    value_policy, iters = value_iteration(problem, delta = tol)\n",
    "    print('Iterations:', iters)\n",
    "    print()\n",
    "\n",
    "    if shape is not None:\n",
    "        print('== Value Iterated Policy ==')\n",
    "        print_policy(value_policy, mapping, shape)\n",
    "        print()\n",
    "\n",
    "\n",
    "    print('== Policy Iteration ==')\n",
    "    policy, iters = policy_iteration(problem, delta = tol)\n",
    "    print('Iterations:', iters)\n",
    "    print()\n",
    "\n",
    "    if shape is not None:\n",
    "        print('== Policy Iterated Policy ==')\n",
    "        print_policy(policy, mapping, shape)\n",
    "        print()\n",
    "\n",
    "    diff = sum([abs(x-y) for x, y in zip(policy.flatten(), value_policy.flatten())])\n",
    "    print('Discrepancy:', diff)\n",
    "    print()\n",
    "\n",
    "    \n",
    "\n",
    "    return policy , value_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy, env_name = None, env = None):\n",
    "    if env == None:\n",
    "        problem = gym.make(env_name)\n",
    "    else:\n",
    "        from gym.wrappers.time_limit import TimeLimit\n",
    "        problem = TimeLimit(env)\n",
    "    \n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    num_steps = []\n",
    "    for i in range(1000):\n",
    "        state = problem.reset()\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done:\n",
    "            #print (\"state is :{} \".format(state))\n",
    "            #print (agent.Q[state])\n",
    "\n",
    "            action = policy[state]\n",
    "            #print (\"action is :{}\".format(action))\n",
    "\n",
    "            state , reward, done, _ = problem.step(action)\n",
    "            i +=1\n",
    "            #problem.render()\n",
    "        #problem.render()\n",
    "        total_reward += reward\n",
    "        num_steps.append(i)\n",
    "\n",
    "    print (\"Total Reward: {}\".format(total_reward))\n",
    "    print (\"Average Steps: {}\".format(np.mean(num_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 17:07:31,802] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== FrozenLake-v0 ==\n",
      "Actions: 4\n",
      "States: 16\n",
      "\n",
      "== Value Iteration ==\n",
      "value_iteration function took 9.016 ms\n",
      "Iterations: 142\n",
      "\n",
      "== Policy Iteration ==\n",
      "policy_iteration function took 16.540 ms\n",
      "Iterations: 2\n",
      "\n",
      "Discrepancy: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapping = {0: \"L\", 1: \"D\", 2: \"R\", 3: \"U\"}\n",
    "shape = (4, 4)\n",
    "frozen_policy, frozen_value_pol = run_discrete('FrozenLake-v0', mapping, shape = None, tol=10**-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 17:07:33,704] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 714.0\n",
      "Average Steps: 41.841\n"
     ]
    }
   ],
   "source": [
    "test_policy(frozen_policy,'FrozenLake-v0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 17:25:09,477] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 754.0\n",
      "Average Steps: 40.181\n"
     ]
    }
   ],
   "source": [
    "test_policy(frozen_value_pol,'FrozenLake-v0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 17:07:36,624] Making new env: FrozenLake8x8-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== FrozenLake8x8-v0 ==\n",
      "Actions: 4\n",
      "States: 64\n",
      "\n",
      "== Value Iteration ==\n",
      "value_iteration function took 8.706 ms\n",
      "Iterations: 66\n",
      "\n",
      "== Policy Iteration ==\n",
      "policy_iteration function took 25.017 ms\n",
      "Iterations: 9\n",
      "\n",
      "Discrepancy: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FROZEN LAKE LARGE\n",
    "shape = (8, 8)\n",
    "froz88_pol , froz88_val_pol = run_discrete('FrozenLake8x8-v0', mapping, shape = None, tol = 10**-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 17:07:38,567] Making new env: FrozenLake8x8-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 716.0\n",
      "Average Steps: 70.681\n"
     ]
    }
   ],
   "source": [
    "test_policy(froz88_pol,'FrozenLake8x8-v0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 17:07:39,424] Making new env: FrozenLake8x8-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 775.0\n",
      "Average Steps: 71.392\n"
     ]
    }
   ],
   "source": [
    "test_policy(froz88_val_pol,'FrozenLake8x8-v0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_Taxi(policy, env_name):\n",
    "    problem = gym.make(env_name)\n",
    "    done = False\n",
    "    total_rewards = []\n",
    "    num_steps = []\n",
    "    for i in range(1000):\n",
    "        state = problem.reset()\n",
    "        done = False\n",
    "        i = 0\n",
    "        ep_reward = 0\n",
    "        while not done:\n",
    "            #print (\"state is :{} \".format(state))\n",
    "            #print (agent.Q[state])\n",
    "\n",
    "            action = policy[state]\n",
    "            #print (\"action is :{}\".format(action))\n",
    "\n",
    "            state , reward, done, _ = problem.step(action)\n",
    "            i +=1\n",
    "            ep_reward+=reward\n",
    "            #problem.render()\n",
    "        #problem.render()\n",
    "        total_rewards.append(ep_reward)\n",
    "        num_steps.append(i)\n",
    "\n",
    "    print (\"Average Episode Reward: {}\".format(np.mean(total_rewards)))\n",
    "    print (\"Average Steps: {}\".format(np.mean(num_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 17:07:40,889] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Taxi-v2 ==\n",
      "Actions: 6\n",
      "States: 500\n",
      "\n",
      "== Value Iteration ==\n",
      "value_iteration function took 870.393 ms\n",
      "Iterations: 117\n",
      "\n",
      "== Policy Iteration ==\n",
      "policy_iteration function took 2129.027 ms\n",
      "Iterations: 16\n",
      "\n",
      "Discrepancy: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapping = {0: \"S\", 1: \"N\", 2: \"E\", 3: \"W\", 4: \"P\", 5: \"D\"}\n",
    "tax_pol, tax_val_pol = run_discrete('Taxi-v2', mapping, tol = 10**-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 17:07:45,112] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 8.558\n",
      "Average Steps: 12.442\n"
     ]
    }
   ],
   "source": [
    "test_policy_Taxi(tax_pol, 'Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 17:07:45,488] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Episode Reward: 8.486\n",
      "Average Steps: 12.514\n"
     ]
    }
   ],
   "source": [
    "test_policy_Taxi(tax_val_pol, 'Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.toy_text import FrozenLakeEnv\n",
    "# NEW 16x16 FROZEN LAKE\n",
    "desc = [\"SFFFFFFFHFFFFFFF\",\n",
    "        \"FFFFFFFFFFFFFFFF\",\n",
    "        \"FFFHFFFFFFFHFFFF\",\n",
    "        \"FFFFFHFFFFFFFHFF\",\n",
    "        \"FFFHFFFFFFFHFFFF\",\n",
    "        \"FHHFFFHFFHHFFFHF\",\n",
    "        \"FHFFHFHFFHFFHFHF\",\n",
    "        \"FFFHFFFFFFFHFFFH\",\n",
    "        \"HFFFFFFFHFFFFFFF\",\n",
    "        \"FFFFFFHHHFFFFFFF\",\n",
    "        \"FFFHFFFFFFFHFFFF\",\n",
    "        \"FFFFFHFFFFFFFHFF\",\n",
    "        \"FFFHFFFFFFFHFFFF\",\n",
    "        \"FHHFFFHFFHHFFFHF\",\n",
    "        \"FHFFHFHFFHFFHFHF\",\n",
    "        \"FFFHFFFFFFFHFFFG\"]\n",
    "frozen16env = FrozenLakeEnv(desc=desc, map_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== FrozenLake16x16v0 ==\n",
      "Actions: 4\n",
      "States: 256\n",
      "\n",
      "== Value Iteration ==\n",
      "value_iteration function took 157.778 ms\n",
      "Iterations: 153\n",
      "\n",
      "== Policy Iteration ==\n",
      "policy_iteration function took 472.609 ms\n",
      "Iterations: 7\n",
      "\n",
      "Discrepancy: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 16x16 Lake!\n",
    "mapping = {0: \"L\", 1: \"D\", 2: \"R\", 3: \"U\"}\n",
    "shape = (16, 16)\n",
    "froz16_pol, froz16_val_pol = run_discrete(\"FrozenLake16x16v0\", mapping, shape = None, env = frozen16env, tol = 10**-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 151.0\n",
      "Average Steps: 112.596\n"
     ]
    }
   ],
   "source": [
    "test_policy(froz16_pol, env = frozen16env )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 112.0\n",
      "Average Steps: 110.059\n"
     ]
    }
   ],
   "source": [
    "test_policy(froz16_val_pol, env=frozen16env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
